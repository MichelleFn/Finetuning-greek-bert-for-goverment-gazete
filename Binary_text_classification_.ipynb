{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichelleFn/Finetuning-greek-bert-for-goverment-gazetes/blob/main/Binary_text_classification_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Installing libraries:**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8HCr3F9D3Q55"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v191elsh2T2m"
      },
      "outputs": [],
      "source": [
        "!pip install unicodedata\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install tensorflow\n",
        "!pip install nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OL9pm3dx29d0"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CpTkuMfV9k_"
      },
      "source": [
        "**Mounting csv file from google drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTFWSBeX26c9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# folder_1='/content/drive/My Drive/csv/csv/dataset_1/' #first dataset \n",
        "folder_2='/content/drive/My Drive/csv/csv/dataset_2/' #second dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfrYTho_WEMI"
      },
      "source": [
        "**Loading Bert tokenizer and TensorFlow model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAEzXlZ829nR"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/bert-base-greek-uncased-v1\")\n",
        "bert = TFAutoModel.from_pretrained(\"nlpaueb/bert-base-greek-uncased-v1\",hidden_dropout_prob=0.5)\n",
        "aug = naw.ContextualWordEmbsAug(model_path='nlpaueb/bert-base-greek-uncased-v1', action=\"substitute\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stop words:**"
      ],
      "metadata": {
        "id": "m01nuLWZ4GBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words  = nltk.corpus.stopwords.words('greek')\n",
        "\n",
        "\n",
        "new_stopwords = ['της','τη','τους','ένας','ενός','ένα','μια','μιας','μιαν','αυτός','αυτή','αυτό','αυτοί', #adding extra stop words that weren't included\n",
        "                 'αυτά','αυτών','ούτος','αυτούς','εκείνος','εκείνη','εκείνο','εκείνοι','εκείνων','εκείνα',\n",
        "                 'ποιος','ποια','ποιοι','ποιων','ποιους','ποιες','πια','είμαι','είναι','είμαστε','είστε',\n",
        "                 'εγώ','εσύ','εμείς','εσείς','άλλος','άλλη','άλλο','άλλων','άλλους','άλλα','κατ’','ως','ή',\n",
        "                 'ούτε','ποτέ','πότε','προς','πρός','υπέρ','άμα','πέρι','οπως','όπως','από','ενώ','συν','πώς'\n",
        "                 ,'εάν','προ','μη','ίσως','κάθε','καθε','ότι','ό,τι','όσο','στα','στους']\n",
        "\n",
        "stop_words.extend(new_stopwords)\n",
        "print(stop_words)\n",
        "\n"
      ],
      "metadata": {
        "id": "Tnl8Nnmt4C3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbDb2kVIWORz"
      },
      "source": [
        "**Class for reading csv files and creating datasets:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text preprocess:**\n"
      ],
      "metadata": {
        "id": "UbDyUcQbvdmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df): #method that makes laters lower case,removes '/n',turns labels binary and removes stop words\n",
        "      \n",
        "  df[\"RawParagraph\"] = df[\"RawParagraph\"].str.lower() \n",
        "    \n",
        "  df = df.replace(r'\\n',' ', regex=True) \n",
        "      \n",
        "  df['RespAPrediction'] = df['RespAPrediction'].replace(  #the extra variations of 'RespA' are because of unresolved typos in the certain csv files       \n",
        "                                                  ['Non-RespA','RespA',' RespA','RespA '],\n",
        "                                                  [0,1,1,1]).astype(int)\n",
        "\n",
        "  df['RawParagraph'] = df['RawParagraph'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)])) \n",
        "  df = df.rename(columns={'RawParagraph': 'text','RespAPrediction': 'label'}) \n",
        "      \n",
        "  return df"
      ],
      "metadata": {
        "id": "jZ_3GhaivD0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating large dataset:**"
      ],
      "metadata": {
        "id": "eQknQxebx0O5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUEaIsr63FRD"
      },
      "outputs": [],
      "source": [
        "def create_dataset(): \n",
        "     \n",
        "  files = os.path.join(folder_2, \"*.csv\")\n",
        "  files = glob.glob(files)\n",
        "  temp_df = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
        "  df=preprocess_data(temp_df)\n",
        "        \n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creates table of csvs:**"
      ],
      "metadata": {
        "id": "kVGEGAnzx7Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_dataset(): #creates a table where at each place there is a pandas dataframe for each csv\n",
        "     \n",
        "  dataframes_list = []\n",
        "  list_of_names =['csva','csvb','csv0','csv1','csv2','csv3','csv4','csv5','csv6','csv7']\n",
        "  col_list=['RawParagraph','RespAPrediction']\n",
        "      \n",
        "  for i in range(len(list_of_names)):\n",
        "\n",
        "    temp_df = pd.read_csv(folder_2+list_of_names[i]+\".csv\",usecols=col_list)\n",
        "    df=preprocess_data(temp_df)\n",
        "    dataframes_list.append(df)\n",
        "\n",
        "  return dataframes_list"
      ],
      "metadata": {
        "id": "0m6dy65pvEd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ja5LMjGV3Fzp"
      },
      "outputs": [],
      "source": [
        "dataframes_list=csv_dataset() #Calling the methods and creating a list \n",
        "text=dataframes_list[0]['text']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(dataframes_list[5])"
      ],
      "metadata": {
        "id": "o_57o7bN9v8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding input text:**"
      ],
      "metadata": {
        "id": "a1_Oj0i06Win"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4COJyIKs2UA"
      },
      "outputs": [],
      "source": [
        "def encode_bert(text): #encodes text to create BERT embeddings (input_ids, attention_masks)\n",
        "\n",
        "  seq_len= 256\n",
        "  num_samples=len(text)\n",
        "  Xids=np.zeros((num_samples,seq_len))\n",
        "  Xmask=np.zeros((num_samples,seq_len))\n",
        "  \n",
        "  for i,sentence in enumerate(text):\n",
        "    \n",
        "    tokens = tokenizer.encode_plus(sentence, max_length=seq_len, truncation=True, \n",
        "                                   padding='max_length',add_special_tokens=True,return_tensors='tf')\n",
        "   \n",
        "    Xids[i,:]=tokens['input_ids']\n",
        "    Xmask[i,:]=tokens['attention_mask']\n",
        "\n",
        "\n",
        "  \n",
        "  inputs = {\n",
        "      'input_ids': Xids,\n",
        "      'attention_mask': Xmask\n",
        "      }\n",
        "  \n",
        "\n",
        "  return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating data combinations:**"
      ],
      "metadata": {
        "id": "ktbkFlsV5jMJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gruaUhIXfZAv"
      },
      "outputs": [],
      "source": [
        "def create_dataset_combinations(fold_number,all_csvs): #creates dataset combination for k fold cross validation\n",
        "  \n",
        "  x_test=0\n",
        "  y_test=0\n",
        "  df= pd.DataFrame(columns=['text','label'])\n",
        "  x_train=[]\n",
        "  y_train=[]\n",
        "  y=0\n",
        "  special_tokens_train=[]\n",
        "\n",
        "  print('FEK',fold_number,'used for testing, the rest for training')\n",
        "  \n",
        "  for csv in all_csvs: \n",
        "    \n",
        "    if y==fold_number: #if the fold loop number is equal to special counter y, the csv is saved for the validation set\n",
        "      x_test=csv['text'].values\n",
        "      y_test=csv['label'].values\n",
        "  \n",
        "    else: #the rest of the csvs are joined together\n",
        "      x_train.append(csv['text'].values)\n",
        "      y_train.append(csv['label'].values)\n",
        "\n",
        "    y=y+1\n",
        "  \n",
        "  x_train=np.concatenate(x_train, axis=0 )\n",
        "  y_train=np.concatenate(y_train,axis=0)\n",
        "  \n",
        "  return x_test,y_test,x_train,y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating model**:"
      ],
      "metadata": {
        "id": "0OQmfd2efkZg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCw2ayWz6WZm"
      },
      "outputs": [],
      "source": [
        "def create_model(optimizer,loss,acc): \n",
        "  \n",
        "  seq_len=256\n",
        "  input_ids=tf.keras.layers.Input(shape=(seq_len,),name='input_ids',dtype='int32')\n",
        "  mask=tf.keras.layers.Input(shape=(seq_len,),name='attention_mask',dtype='int32')\n",
        "\n",
        "  embeddings=bert.bert(input_ids,attention_mask=mask)[1]\n",
        " \n",
        "  x = tf.keras.layers.Dense(512,activation='relu')(embeddings)\n",
        "  layer2 = tf.keras.layers.Dropout(0.4)(x)\n",
        "\n",
        "  y = tf.keras.layers.Dense(2,activation='sigmoid',name='outputs')(layer2)\n",
        "  model=tf.keras.Model(inputs=[input_ids,mask],outputs=y)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qnA_PM5UNzr"
      },
      "outputs": [],
      "source": [
        "optimizer= tf.keras.optimizers.Adam(learning_rate=0.0001,decay=1e-6)\n",
        "loss=tf.keras.losses.BinaryCrossentropy()\n",
        "acc=tf.keras.metrics.BinaryAccuracy('accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErNZd39CNmYp"
      },
      "source": [
        "\n",
        " **10 fold cross validation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsQIozO9i7U9"
      },
      "outputs": [],
      "source": [
        "def training_model(fold_i):\n",
        "  \n",
        "  i=fold_i #i=0-9 the loop doesn't work because the model doesn't get deleted so the collab Runtime has to be restarted\n",
        "\n",
        "  x_val,y_val,x_train,y_train= create_dataset_combinations(i,dataframes_list)\n",
        "\n",
        "  print('Size of training set: ',len(x_train))\n",
        "  print('Size of test set: ', len(x_val))\n",
        "\n",
        "  val_inputs=encode_bert(x_val) # return BERT embeddings\n",
        "  val_labels=np.zeros((len(y_val),y_val.max()+1)) #on-hot encode labels\n",
        "  val_labels[np.arange(len(y_val)),y_val] = 1\n",
        "  \n",
        "  train_inputs=encode_bert(x_train)\n",
        "  train_labels=np.zeros((len(y_train),y_train.max()+1))\n",
        "  train_labels[np.arange(len(y_train)),y_train] = 1\n",
        "  \n",
        "\n",
        "  model = create_model(optimizer,loss,acc)\n",
        "  model.fit(train_inputs, train_labels,epochs=3,batch_size=16, validation_data=(val_inputs,val_labels))\n",
        "  results=model.evaluate(val_inputs,val_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_model(0) #enter manually 0-9"
      ],
      "metadata": {
        "id": "4wZEKWccuEfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAZvZHNILsKk"
      },
      "source": [
        "**Stratified 10 cross fold validation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVYYu0LIOkc_"
      },
      "outputs": [],
      "source": [
        "df=create_dataset() #only one big pandas dataset is needed for this version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL6gnltConAc"
      },
      "outputs": [],
      "source": [
        "X_data=df['text']\n",
        "Y_data=df['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G28076P1nof9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "#9 fold cross validation with complete dataset that is split into train and test each time\n",
        "skf = StratifiedKFold(n_splits=10,shuffle=True, random_state=1)\n",
        "\n",
        "\n",
        "a=0\n",
        "score=[]\n",
        "for train_index, test_index in skf.split(X_data, Y_data):\n",
        "    \n",
        "    print(f\"Generating Inputs for fold {a}\")\n",
        "   \n",
        "    \n",
        "    \n",
        "    train_X, test_X = X_data[train_index], X_data[test_index]\n",
        "    train_y, test_y = Y_data[train_index], Y_data[test_index]\n",
        "\n",
        "    train_inputs=encode_bert(train_X)\n",
        "\n",
        "    \n",
        "    train_label_array=train_y.values\n",
        "\n",
        "    \n",
        "    train_labels=np.zeros((len(train_y),train_label_array.max()+1))\n",
        "    train_labels[np.arange(len(train_y)),train_label_array] = 1\n",
        "    \n",
        "\n",
        "    \n",
        "    val_inputs=encode_bert(test_X)\n",
        "    \n",
        "    test_label_array=test_y.values\n",
        "    test_labels=np.zeros((len(test_y),test_label_array.max()+1))\n",
        "    test_labels[np.arange(len(test_y)),test_label_array] = 1\n",
        "    \n",
        "  \n",
        "    model = create_model(optimizer,loss,acc)\n",
        "    early_stopping=tf.keras.callbacks.EarlyStopping(patience=5)\n",
        "    model.fit(train_inputs, train_labels,epochs=5,batch_size=8,\n",
        "              validation_data=(val_inputs,test_labels),callbacks=early_stopping)\n",
        "    results=model.evaluate(val_inputs,test_labels)\n",
        " \n",
        "    a+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtZaO3MHiGCA"
      },
      "outputs": [],
      "source": [
        "#for testing purposes,only the first fek as testing and the rest for training\n",
        "seq_len= 512\n",
        "d3 = pd.concat([dataframes_list[0],dataframes_list[1],dataframes_list[2],dataframes_list[3],dataframes_list[4],dataframes_list[5],dataframes_list[6],dataframes_list[7]])\n",
        "print(d3)\n",
        "train_inputs=encode_bert(d3['text'])\n",
        "print(train_inputs)\n",
        "train_y=d3['label']\n",
        "\n",
        "label_array=train_y.values\n",
        "print(label_array)\n",
        "len(label_array)\n",
        "\n",
        "train_label_array=train_y\n",
        "train_labels=np.zeros((len(train_y),train_label_array.max()+1))\n",
        "train_labels[np.arange(len(train_y)),train_label_array] = 1\n",
        "print(train_labels)\n",
        "test_inputs=encode_bert(dataframes_list[8]['text'])\n",
        "test_y=d3['label']\n",
        "test_label_array=test_y\n",
        "test_labels=np.zeros((len(test_y),test_label_array.max()+1))\n",
        "test_labels[np.arange(len(test_y)),test_label_array] = 1\n",
        "\n",
        "early_stopping=tf.keras.callbacks.EarlyStopping(patience=5)\n",
        "model = create_model(optimizer,loss,acc)\n",
        "model.fit(train_inputs, train_labels,epochs=5,batch_size=8,\n",
        "              validation_data=(test_inputs,test_labels),callbacks=early_stopping)\n",
        "results=model.evaluate(val_inputs,test_labels)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}