{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichelleFn/Finetuning-greek-bert-for-goverment-gazetes/blob/main/Binary_text_classification_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v191elsh2T2m"
      },
      "outputs": [],
      "source": [
        "!pip install unicodedata\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install tensorflow\n",
        "!pip install nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OL9pm3dx29d0"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import nlpaug.augmenter.word as naw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CpTkuMfV9k_"
      },
      "source": [
        "**Mounting csv file from google drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTFWSBeX26c9",
        "outputId": "7601f662-1dea-4d8b-b67b-be0a5be257c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/My Drive/csv/csv/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfrYTho_WEMI"
      },
      "source": [
        "**Loading Bert tokenizer and TensorFlow model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAEzXlZ829nR"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/bert-base-greek-uncased-v1\")\n",
        "bert = TFAutoModel.from_pretrained(\"nlpaueb/bert-base-greek-uncased-v1\",hidden_dropout_prob=0.5)\n",
        "aug = naw.ContextualWordEmbsAug(model_path='nlpaueb/bert-base-greek-uncased-v1', action=\"substitute\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbDb2kVIWORz"
      },
      "source": [
        "**Class for reading csv files and creating datasets:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HUEaIsr63FRD"
      },
      "outputs": [],
      "source": [
        "class csv_merge:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def create_dataset(self): #creates one big dataset out of all csvs in drive \n",
        "     \n",
        "        \n",
        "        files = os.path.join(folder, \"*.csv\")\n",
        "        files = glob.glob(files)\n",
        "        temp_df = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
        "        df=self.preprocess_data(temp_df)\n",
        "        \n",
        "        return df\n",
        "\n",
        "    def preprocess_data(self,df):\n",
        "      \n",
        "      df[\"RawParagraph\"] = df[\"RawParagraph\"].str.lower() #makes all letters lower case since we are using Bert uncased\n",
        "      df = df.replace(r'\\n',' ', regex=True) #removes /n\n",
        "  \n",
        "      df['RespAPrediction'] = df['RespAPrediction'].replace(['Non-RespA','RespA',' RespA','RespA '],[0,1,1,1]).astype(int) #makes labels 0 or 1\n",
        "      df = df.rename(columns={'RawParagraph': 'text','RespAPrediction': 'label'})  #renames dataset collumns\n",
        "      \n",
        "      return df\n",
        "\n",
        "    def csv_dataset(self): #returns a list of datasets that contain each csv\n",
        "     \n",
        "      dataframes_list = []\n",
        "\n",
        "      list_of_names =['csva','csvb','csvc','csvd','csve','csv0','csv1','csv3','csv4','csv5','csv6','csv7']#,'csv8','csv9','csv10']#,'results20080100058','results20080100036']#,'csv11','csv12']#,'csv13','csv14','csv15']#]\n",
        "      \n",
        "      col_list=['RawParagraph','RespAPrediction'] #the column names that are neccesary\n",
        "      \n",
        "      for i in range(len(list_of_names)):\n",
        "\n",
        "        temp_df = pd.read_csv(folder+list_of_names[i]+\".csv\",usecols=col_list)\n",
        "        df=self.preprocess_data(temp_df)\n",
        "        dataframes_list.append(df)\n",
        "\n",
        "      \n",
        "\n",
        "      return dataframes_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TLV2poxWn1s"
      },
      "source": [
        "**Creating list of datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jMoXnm4TOb3U"
      },
      "outputs": [],
      "source": [
        "csv_merge =  csv_merge()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Ja5LMjGV3Fzp"
      },
      "outputs": [],
      "source": [
        " #Calling the methods and creating a list \n",
        "dataframes_list=csv_merge.csv_dataset()\n",
        "text=dataframes_list[0]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gruaUhIXfZAv"
      },
      "outputs": [],
      "source": [
        "def create_dataset_combinations(fold_loop_number,all_csvs): #Splitting csvs into train and test\n",
        "  \n",
        "  x_test=0\n",
        "  y_test=0\n",
        "  df= pd.DataFrame(columns=['text','label'])\n",
        "  x_train=[]\n",
        "  y_train=[]\n",
        "  y=0\n",
        "  special_tokens_train=[]\n",
        "\n",
        "  print('FEK',fold_loop_number,'used for testing, the rest for training')\n",
        "  for csv in all_csvs: #iterating through each dataset in the list\n",
        "    if y==fold_loop_number: #choosing the test dataset\n",
        "      x_test=csv['text'].values\n",
        "      y_test=csv['label'].values\n",
        "  \n",
        "    else: #the remaining datasets are merged into one\n",
        "     \n",
        "      x_train.append(csv['text'].values)\n",
        "      y_train.append(csv['label'].values)\n",
        "\n",
        "    y=y+1\n",
        "    \n",
        "  x_train=np.concatenate(x_train, axis=0 )\n",
        "  y_train=np.concatenate(y_train,axis=0)\n",
        "  \n",
        "  return x_test,y_test,x_train,y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "j4COJyIKs2UA"
      },
      "outputs": [],
      "source": [
        "def encode_bert(text): #encoding text for Bert\n",
        "\n",
        "  seq_len= 256\n",
        "  num_samples=len(text)\n",
        "\n",
        "  #defining 2 empty numpy arrays for the input_ids and attention_mask\n",
        "  Xids=np.zeros((num_samples,seq_len))\n",
        "  Xmask=np.zeros((num_samples,seq_len))\n",
        "  \n",
        "  for i,sentence in enumerate(text):\n",
        "\n",
        "    tokens = tokenizer.encode_plus(sentence, max_length=seq_len, truncation=True, padding='max_length',add_special_tokens=True,return_tensors='tf')#additional_special_tokens=)\n",
        "    # print(sentence)\n",
        "    # print(tokens['input_ids'])\n",
        "    Xids[i,:]=tokens['input_ids']\n",
        "    Xmask[i,:]=tokens['attention_mask']\n",
        "\n",
        "\n",
        "  \n",
        "  inputs = {\n",
        "      'input_ids': Xids,\n",
        "      'attention_mask': Xmask\n",
        "      }\n",
        "  \n",
        "\n",
        "  return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0qnA_PM5UNzr"
      },
      "outputs": [],
      "source": [
        "optimizer= tf.keras.optimizers.Adam(lr=0.0001,decay=1e-6)\n",
        "loss=tf.keras.losses.BinaryCrossentropy()\n",
        "acc=tf.keras.metrics.BinaryAccuracy('accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "QCw2ayWz6WZm"
      },
      "outputs": [],
      "source": [
        "def create_model(optimizer,loss,acc): #Bert model\n",
        "  seq_len=256\n",
        "  input_ids=tf.keras.layers.Input(shape=(seq_len,),name='input_ids',dtype='int32')\n",
        "  mask=tf.keras.layers.Input(shape=(seq_len,),name='attention_mask',dtype='int32')\n",
        "\n",
        "  embeddings=bert.bert(input_ids,attention_mask=mask)[1]\n",
        "  # layer = tf.keras.layers.Dropout(0.4)(embeddings)\n",
        "\n",
        "  x = tf.keras.layers.Dense(512,activation='relu')(embeddings)\n",
        "  layer2 = tf.keras.layers.Dropout(0.5)(x)\n",
        "\n",
        "  y = tf.keras.layers.Dense(2,activation='sigmoid',name='outputs')(layer2)\n",
        "  model=tf.keras.Model(inputs=[input_ids,mask],outputs=y)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErNZd39CNmYp"
      },
      "source": [
        "**9 fold cross validation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsQIozO9i7U9"
      },
      "outputs": [],
      "source": [
        "for i in range(7):\n",
        "  \n",
        "  tf.keras.backend.clear_session()\n",
        "  x_test,y_test,x_train,y_train= create_dataset_combinations(i,dataframes_list)\n",
        "\n",
        "  print('Size of training set: ',len(x_train))\n",
        "  print('Size of test set: ', len(x_test))\n",
        "\n",
        "  val_inputs=encode_bert(x_test) \n",
        "  test_labels=np.zeros((len(y_test),y_test.max()+1))\n",
        "  test_labels[np.arange(len(y_test)),y_test] = 1\n",
        "  \n",
        "  train_inputs=encode_bert(x_train)\n",
        "\n",
        "\n",
        "  train_label_array=y_train\n",
        "  train_labels=np.zeros((len(y_train),y_train.max()+1))\n",
        "  train_labels[np.arange(len(y_train)),y_train] = 1\n",
        "  \n",
        "\n",
        "  model = create_model(optimizer,loss,acc)\n",
        "  model.fit(train_inputs, train_labels,epochs=3,batch_size=16, validation_data=(val_inputs,test_labels))\n",
        "  results=model.evaluate(val_inputs,test_labels)\n",
        "  # del model\n",
        "  # K.clear_session()\n",
        "  # tf.reset_default_graph()\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajqeV-SIfCTC"
      },
      "outputs": [],
      "source": [
        "tf.compat.v1.reset_default_graph()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfYg7s0GX20i"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GQTb7e20MB0"
      },
      "outputs": [],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PVZoXKbbQR7"
      },
      "outputs": [],
      "source": [
        "model.reset_states()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAZvZHNILsKk"
      },
      "source": [
        "**Alternative method for 9 cross fold validation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVYYu0LIOkc_"
      },
      "outputs": [],
      "source": [
        "df= csv_merge.create_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL6gnltConAc"
      },
      "outputs": [],
      "source": [
        "X_data=df['text']\n",
        "Y_data=df['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G28076P1nof9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "#9 fold cross validation with complete dataset that is split into train and test each time\n",
        "skf = StratifiedKFold(n_splits=9,shuffle=True, random_state=1)\n",
        "\n",
        "\n",
        "a=0\n",
        "score=[]\n",
        "for train_index, test_index in skf.split(X_data, Y_data):\n",
        "    \n",
        "    print(f\"Generating Inputs for fold {a}\")\n",
        "   \n",
        "    \n",
        "    \n",
        "    train_X, test_X = X_data[train_index], X_data[test_index]\n",
        "    train_y, test_y = Y_data[train_index], Y_data[test_index]\n",
        "\n",
        "    train_inputs=encode_bert(train_X)\n",
        "\n",
        "    \n",
        "    train_label_array=train_y.values\n",
        "\n",
        "    \n",
        "    train_labels=np.zeros((len(train_y),train_label_array.max()+1))\n",
        "    train_labels[np.arange(len(train_y)),train_label_array] = 1\n",
        "    \n",
        "\n",
        "    \n",
        "    val_inputs=encode_bert(test_X)\n",
        "    \n",
        "    test_label_array=test_y.values\n",
        "    test_labels=np.zeros((len(test_y),test_label_array.max()+1))\n",
        "    test_labels[np.arange(len(test_y)),test_label_array] = 1\n",
        "    \n",
        "  \n",
        "    model = create_model(optimizer,loss,acc)\n",
        "    early_stopping=tf.keras.callbacks.EarlyStopping(patience=5)\n",
        "    model.fit(train_inputs, train_labels,epochs=5,batch_size=8,\n",
        "              validation_data=(val_inputs,test_labels),callbacks=early_stopping)\n",
        "    results=model.evaluate(val_inputs,test_labels)\n",
        "  \n",
        "\n",
        "    \n",
        "    a+=1\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtZaO3MHiGCA"
      },
      "outputs": [],
      "source": [
        "#for testing purposes,only the first fek as testing and the rest for training\n",
        "seq_len= 512\n",
        "d3 = pd.concat([dataframes_list[0],dataframes_list[1],dataframes_list[2],dataframes_list[3],dataframes_list[4],dataframes_list[5],dataframes_list[6],dataframes_list[7]])\n",
        "print(d3)\n",
        "train_inputs=encode_bert(d3['text'])\n",
        "print(train_inputs)\n",
        "train_y=d3['label']\n",
        "\n",
        "label_array=train_y.values\n",
        "print(label_array)\n",
        "len(label_array)\n",
        "\n",
        "train_label_array=train_y\n",
        "train_labels=np.zeros((len(train_y),train_label_array.max()+1))\n",
        "train_labels[np.arange(len(train_y)),train_label_array] = 1\n",
        "print(train_labels)\n",
        "test_inputs=encode_bert(dataframes_list[8]['text'])\n",
        "test_y=d3['label']\n",
        "test_label_array=test_y\n",
        "test_labels=np.zeros((len(test_y),test_label_array.max()+1))\n",
        "test_labels[np.arange(len(test_y)),test_label_array] = 1\n",
        "\n",
        "early_stopping=tf.keras.callbacks.EarlyStopping(patience=5)\n",
        "model = create_model(optimizer,loss,acc)\n",
        "model.fit(train_inputs, train_labels,epochs=5,batch_size=8,\n",
        "              validation_data=(test_inputs,test_labels),callbacks=early_stopping)\n",
        "results=model.evaluate(val_inputs,test_labels)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Binary_text_classification .ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}