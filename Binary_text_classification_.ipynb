{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichelleFn/Finetuning-greek-bert-for-goverment-gazetes/blob/main/Binary_text_classification_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Installing libraries:**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8HCr3F9D3Q55"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v191elsh2T2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae9ef46-82b8-4fe1-f76b-cf5ad9f39a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement unicodedata (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for unicodedata\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.47.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0.7)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nlpaug in /usr/local/lib/python3.7/dist-packages (1.1.11)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (1.21.6)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (1.3.5)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (4.4.0)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown>=4.0.0->nlpaug) (1.15.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown>=4.0.0->nlpaug) (4.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown>=4.0.0->nlpaug) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown>=4.0.0->nlpaug) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2022.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install unicodedata\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install tensorflow\n",
        "!pip install nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OL9pm3dx29d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57a19171-9929-4074-d2b6-3452dc6f00a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import unicodedata\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CpTkuMfV9k_"
      },
      "source": [
        "**Mounting csv file from google drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gTFWSBeX26c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "902c7c4b-b9d0-47c6-8e63-f80d1113a7f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# folder_1='/content/drive/My Drive/csv/csv/dataset_1/' #first dataset \n",
        "folder_2='/content/drive/My Drive/csv/csv/dataset_2/' #second dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfrYTho_WEMI"
      },
      "source": [
        "**Loading Bert tokenizer and TensorFlow model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SAEzXlZ829nR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0baa0a12-79e5-4bf9-838e-ca891a9b11fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at nlpaueb/bert-base-greek-uncased-v1 were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at nlpaueb/bert-base-greek-uncased-v1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/bert-base-greek-uncased-v1\")\n",
        "bert = TFAutoModel.from_pretrained(\"nlpaueb/bert-base-greek-uncased-v1\",hidden_dropout_prob=0.5)\n",
        "aug = naw.ContextualWordEmbsAug(model_path='nlpaueb/bert-base-greek-uncased-v1', action=\"substitute\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stop words:**"
      ],
      "metadata": {
        "id": "m01nuLWZ4GBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words  = nltk.corpus.stopwords.words('greek')\n",
        "\n",
        "\n",
        "new_stopwords = ['της','τη','τους','ένας','ενός','ένα','μια','μιας','μιαν','αυτός','αυτή','αυτό','αυτοί', #adding extra stop words that weren't included\n",
        "                 'αυτά','αυτών','ούτος','αυτούς','εκείνος','εκείνη','εκείνο','εκείνοι','εκείνων','εκείνα',\n",
        "                 'ποιος','ποια','ποιοι','ποιων','ποιους','ποιες','πια','είμαι','είναι','είμαστε','είστε',\n",
        "                 'εγώ','εσύ','εμείς','εσείς','άλλος','άλλη','άλλο','άλλων','άλλους','άλλα','κατ’','ως','ή',\n",
        "                 'ούτε','ποτέ','πότε','προς','πρός','υπέρ','άμα','πέρι','οπως','όπως','από','ενώ','συν','πώς'\n",
        "                 ,'εάν','προ','μη','ίσως','κάθε','καθε','ότι','ό,τι','όσο','στα','στους']\n",
        "\n",
        "stop_words.extend(new_stopwords)\n",
        "print(stop_words)\n",
        "\n"
      ],
      "metadata": {
        "id": "Tnl8Nnmt4C3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42b9f617-0a2e-4e65-d1fa-ee17f78f0cd4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['αλλα', 'αν', 'αντι', 'απο', 'αυτα', 'αυτεσ', 'αυτη', 'αυτο', 'αυτοι', 'αυτοσ', 'αυτουσ', 'αυτων', 'αἱ', 'αἳ', 'αἵ', 'αὐτόσ', 'αὐτὸς', 'αὖ', 'γάρ', 'γα', 'γα^', 'γε', 'για', 'γοῦν', 'γὰρ', \"δ'\", 'δέ', 'δή', 'δαί', 'δαίσ', 'δαὶ', 'δαὶς', 'δε', 'δεν', \"δι'\", 'διά', 'διὰ', 'δὲ', 'δὴ', 'δ’', 'εαν', 'ειμαι', 'ειμαστε', 'ειναι', 'εισαι', 'ειστε', 'εκεινα', 'εκεινεσ', 'εκεινη', 'εκεινο', 'εκεινοι', 'εκεινοσ', 'εκεινουσ', 'εκεινων', 'ενω', 'επ', 'επι', 'εἰ', 'εἰμί', 'εἰμὶ', 'εἰς', 'εἰσ', 'εἴ', 'εἴμι', 'εἴτε', 'η', 'θα', 'ισωσ', 'κ', 'καί', 'καίτοι', 'καθ', 'και', 'κατ', 'κατά', 'κατα', 'κατὰ', 'καὶ', 'κι', 'κἀν', 'κἂν', 'μέν', 'μή', 'μήτε', 'μα', 'με', 'μεθ', 'μετ', 'μετά', 'μετα', 'μετὰ', 'μη', 'μην', 'μἐν', 'μὲν', 'μὴ', 'μὴν', 'να', 'ο', 'οι', 'ομωσ', 'οπωσ', 'οσο', 'οτι', 'οἱ', 'οἳ', 'οἷς', 'οὐ', 'οὐδ', 'οὐδέ', 'οὐδείσ', 'οὐδεὶς', 'οὐδὲ', 'οὐδὲν', 'οὐκ', 'οὐχ', 'οὐχὶ', 'οὓς', 'οὔτε', 'οὕτω', 'οὕτως', 'οὕτωσ', 'οὖν', 'οὗ', 'οὗτος', 'οὗτοσ', 'παρ', 'παρά', 'παρα', 'παρὰ', 'περί', 'περὶ', 'ποια', 'ποιεσ', 'ποιο', 'ποιοι', 'ποιοσ', 'ποιουσ', 'ποιων', 'ποτε', 'που', 'ποῦ', 'προ', 'προσ', 'πρόσ', 'πρὸ', 'πρὸς', 'πως', 'πωσ', 'σε', 'στη', 'στην', 'στο', 'στον', 'σόσ', 'σύ', 'σύν', 'σὸς', 'σὺ', 'σὺν', 'τά', 'τήν', 'τί', 'τίς', 'τίσ', 'τα', 'ταῖς', 'τε', 'την', 'τησ', 'τι', 'τινα', 'τις', 'τισ', 'το', 'τοί', 'τοι', 'τοιοῦτος', 'τοιοῦτοσ', 'τον', 'τοτε', 'του', 'τούσ', 'τοὺς', 'τοῖς', 'τοῦ', 'των', 'τό', 'τόν', 'τότε', 'τὰ', 'τὰς', 'τὴν', 'τὸ', 'τὸν', 'τῆς', 'τῆσ', 'τῇ', 'τῶν', 'τῷ', 'ωσ', \"ἀλλ'\", 'ἀλλά', 'ἀλλὰ', 'ἀλλ’', 'ἀπ', 'ἀπό', 'ἀπὸ', 'ἀφ', 'ἂν', 'ἃ', 'ἄλλος', 'ἄλλοσ', 'ἄν', 'ἄρα', 'ἅμα', 'ἐάν', 'ἐγώ', 'ἐγὼ', 'ἐκ', 'ἐμόσ', 'ἐμὸς', 'ἐν', 'ἐξ', 'ἐπί', 'ἐπεὶ', 'ἐπὶ', 'ἐστι', 'ἐφ', 'ἐὰν', 'ἑαυτοῦ', 'ἔτι', 'ἡ', 'ἢ', 'ἣ', 'ἤ', 'ἥ', 'ἧς', 'ἵνα', 'ὁ', 'ὃ', 'ὃν', 'ὃς', 'ὅ', 'ὅδε', 'ὅθεν', 'ὅπερ', 'ὅς', 'ὅσ', 'ὅστις', 'ὅστισ', 'ὅτε', 'ὅτι', 'ὑμόσ', 'ὑπ', 'ὑπέρ', 'ὑπό', 'ὑπὲρ', 'ὑπὸ', 'ὡς', 'ὡσ', 'ὥς', 'ὥστε', 'ὦ', 'ᾧ', 'της', 'τη', 'τους', 'ένας', 'ενός', 'ένα', 'μια', 'μιας', 'μιαν', 'αυτός', 'αυτή', 'αυτό', 'αυτοί', 'αυτά', 'αυτών', 'ούτος', 'αυτούς', 'εκείνος', 'εκείνη', 'εκείνο', 'εκείνοι', 'εκείνων', 'εκείνα', 'ποιος', 'ποια', 'ποιοι', 'ποιων', 'ποιους', 'ποιες', 'πια', 'είμαι', 'είναι', 'είμαστε', 'είστε', 'εγώ', 'εσύ', 'εμείς', 'εσείς', 'άλλος', 'άλλη', 'άλλο', 'άλλων', 'άλλους', 'άλλα', 'κατ’', 'ως', 'ή', 'ούτε', 'ποτέ', 'πότε', 'προς', 'πρός', 'υπέρ', 'άμα', 'πέρι', 'οπως', 'όπως', 'από', 'ενώ', 'συν', 'πώς', 'εάν', 'προ', 'μη', 'ίσως', 'κάθε', 'καθε', 'ότι', 'ό,τι', 'όσο', 'στα', 'στους']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbDb2kVIWORz"
      },
      "source": [
        "**Class for reading csv files and creating datasets:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text preprocess:**\n"
      ],
      "metadata": {
        "id": "UbDyUcQbvdmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df): #method that makes laters lower case,removes '/n',turns labels binary and removes stop words\n",
        "      \n",
        "  df[\"RawParagraph\"] = df[\"RawParagraph\"].str.lower() \n",
        "    \n",
        "  df = df.replace(r'\\n',' ', regex=True) \n",
        "      \n",
        "  df['RespAPrediction'] = df['RespAPrediction'].replace(  #the extra variations of 'RespA' are because of unresolved typos in the certain csv files       \n",
        "                                                  ['Non-RespA','RespA',' RespA','RespA '],\n",
        "                                                  [0,1,1,1]).astype(int)\n",
        "\n",
        "  df['RawParagraph'] = df['RawParagraph'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)])) \n",
        "  df = df.rename(columns={'RawParagraph': 'text','RespAPrediction': 'label'}) \n",
        "      \n",
        "  return df"
      ],
      "metadata": {
        "id": "jZ_3GhaivD0w"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating large dataset:**"
      ],
      "metadata": {
        "id": "eQknQxebx0O5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HUEaIsr63FRD"
      },
      "outputs": [],
      "source": [
        "def create_dataset(): \n",
        "     \n",
        "  files = os.path.join(folder_2, \"*.csv\")\n",
        "  files = glob.glob(files)\n",
        "  temp_df = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
        "  df=preprocess_data(temp_df)\n",
        "        \n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creates table of csvs:**"
      ],
      "metadata": {
        "id": "kVGEGAnzx7Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_dataset(): #creates a table where at each place there is a pandas dataframe for each csv\n",
        "     \n",
        "  dataframes_list = []\n",
        "  list_of_names =['csva','csvb','csv0','csv1','csv2','csv3','csv4','csv5','csv6','csv7']\n",
        "  col_list=['RawParagraph','RespAPrediction']\n",
        "      \n",
        "  for i in range(len(list_of_names)):\n",
        "\n",
        "    temp_df = pd.read_csv(folder_2+list_of_names[i]+\".csv\",usecols=col_list)\n",
        "    df=preprocess_data(temp_df)\n",
        "    dataframes_list.append(df)\n",
        "\n",
        "  return dataframes_list"
      ],
      "metadata": {
        "id": "0m6dy65pvEd5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ja5LMjGV3Fzp"
      },
      "outputs": [],
      "source": [
        "dataframes_list=csv_dataset() #Calling the methods and creating a list \n",
        "text=dataframes_list[0]['text']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(dataframes_list[5])"
      ],
      "metadata": {
        "id": "o_57o7bN9v8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding input text:**"
      ],
      "metadata": {
        "id": "a1_Oj0i06Win"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "j4COJyIKs2UA"
      },
      "outputs": [],
      "source": [
        "def encode_bert(text): #encodes text to create BERT embeddings (input_ids, attention_masks)\n",
        "\n",
        "  seq_len= 256\n",
        "  num_samples=len(text)\n",
        "  Xids=np.zeros((num_samples,seq_len))\n",
        "  Xmask=np.zeros((num_samples,seq_len))\n",
        "  \n",
        "  for i,sentence in enumerate(text):\n",
        "    \n",
        "    tokens = tokenizer.encode_plus(sentence, max_length=seq_len, truncation=True, \n",
        "                                   padding='max_length',add_special_tokens=True,return_tensors='tf')\n",
        "   \n",
        "    Xids[i,:]=tokens['input_ids']\n",
        "    Xmask[i,:]=tokens['attention_mask']\n",
        "\n",
        "\n",
        "  \n",
        "  inputs = {\n",
        "      'input_ids': Xids,\n",
        "      'attention_mask': Xmask\n",
        "      }\n",
        "  \n",
        "\n",
        "  return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating data combinations:**"
      ],
      "metadata": {
        "id": "ktbkFlsV5jMJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gruaUhIXfZAv"
      },
      "outputs": [],
      "source": [
        "def create_dataset_combinations(fold_number,all_csvs): #creates dataset combination for k fold cross validation\n",
        "  \n",
        "  x_test=0\n",
        "  y_test=0\n",
        "  df= pd.DataFrame(columns=['text','label'])\n",
        "  x_train=[]\n",
        "  y_train=[]\n",
        "  y=0\n",
        "  special_tokens_train=[]\n",
        "\n",
        "  print('FEK',fold_number,'used for testing, the rest for training')\n",
        "  \n",
        "  for csv in all_csvs: \n",
        "    \n",
        "    if y==fold_number: #if the fold loop number is equal to special counter y, the csv is saved for the validation set\n",
        "      x_test=csv['text'].values\n",
        "      y_test=csv['label'].values\n",
        "  \n",
        "    else: #the rest of the csvs are joined together\n",
        "      x_train.append(csv['text'].values)\n",
        "      y_train.append(csv['label'].values)\n",
        "\n",
        "    y=y+1\n",
        "  \n",
        "  x_train=np.concatenate(x_train, axis=0 )\n",
        "  y_train=np.concatenate(y_train,axis=0)\n",
        "  \n",
        "  return x_test,y_test,x_train,y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating model**:"
      ],
      "metadata": {
        "id": "0OQmfd2efkZg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QCw2ayWz6WZm"
      },
      "outputs": [],
      "source": [
        "def create_model(optimizer,loss,acc): \n",
        "  \n",
        "  seq_len=256\n",
        "  input_ids=tf.keras.layers.Input(shape=(seq_len,),name='input_ids',dtype='int32')\n",
        "  mask=tf.keras.layers.Input(shape=(seq_len,),name='attention_mask',dtype='int32')\n",
        "\n",
        "  embeddings=bert.bert(input_ids,attention_mask=mask)[1]\n",
        " \n",
        "  x = tf.keras.layers.Dense(512,activation='relu')(embeddings)\n",
        "  layer2 = tf.keras.layers.Dropout(0.4)(x)\n",
        "\n",
        "  y = tf.keras.layers.Dense(2,activation='sigmoid',name='outputs')(layer2)\n",
        "  model=tf.keras.Model(inputs=[input_ids,mask],outputs=y)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0qnA_PM5UNzr"
      },
      "outputs": [],
      "source": [
        "optimizer= tf.keras.optimizers.Adam(learning_rate=0.0001,decay=1e-6)\n",
        "loss=tf.keras.losses.BinaryCrossentropy()\n",
        "acc=tf.keras.metrics.BinaryAccuracy('accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErNZd39CNmYp"
      },
      "source": [
        "\n",
        " **10 fold cross validation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nsQIozO9i7U9"
      },
      "outputs": [],
      "source": [
        "def training_model(fold_i):\n",
        "  \n",
        "  i=fold_i #i=0-9 the loop doesn't work because the model doesn't get deleted so the collab Runtime has to be restarted\n",
        "\n",
        "  x_val,y_val,x_train,y_train= create_dataset_combinations(i,dataframes_list)\n",
        "\n",
        "  print('Size of training set: ',len(x_train))\n",
        "  print('Size of test set: ', len(x_val))\n",
        "\n",
        "  val_inputs=encode_bert(x_val) # return BERT embeddings\n",
        "  val_labels=np.zeros((len(y_val),y_val.max()+1)) #on-hot encode labels\n",
        "  val_labels[np.arange(len(y_val)),y_val] = 1\n",
        "  \n",
        "  train_inputs=encode_bert(x_train)\n",
        "  train_labels=np.zeros((len(y_train),y_train.max()+1))\n",
        "  train_labels[np.arange(len(y_train)),y_train] = 1\n",
        "  \n",
        "\n",
        "  model = create_model(optimizer,loss,acc)\n",
        "  model.fit(train_inputs, train_labels,epochs=3,batch_size=16, validation_data=(val_inputs,val_labels))\n",
        "  results=model.evaluate(val_inputs,val_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_model(0) #enter manually 0-9"
      ],
      "metadata": {
        "id": "4wZEKWccuEfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAZvZHNILsKk"
      },
      "source": [
        "**Stratified 10 cross fold validation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVYYu0LIOkc_"
      },
      "outputs": [],
      "source": [
        "df=create_dataset() #only one big pandas dataset is needed for this version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL6gnltConAc"
      },
      "outputs": [],
      "source": [
        "X_data=df['text']\n",
        "Y_data=df['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G28076P1nof9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "#9 fold cross validation with complete dataset that is split into train and test each time\n",
        "skf = StratifiedKFold(n_splits=10,shuffle=True, random_state=1)\n",
        "\n",
        "\n",
        "a=0\n",
        "score=[]\n",
        "for train_index, test_index in skf.split(X_data, Y_data):\n",
        "    \n",
        "    print(f\"Generating Inputs for fold {a}\")\n",
        "   \n",
        "    \n",
        "    \n",
        "    train_X, test_X = X_data[train_index], X_data[test_index]\n",
        "    train_y, test_y = Y_data[train_index], Y_data[test_index]\n",
        "\n",
        "    train_inputs=encode_bert(train_X)\n",
        "\n",
        "    \n",
        "    train_label_array=train_y.values\n",
        "\n",
        "    \n",
        "    train_labels=np.zeros((len(train_y),train_label_array.max()+1))\n",
        "    train_labels[np.arange(len(train_y)),train_label_array] = 1\n",
        "    \n",
        "\n",
        "    \n",
        "    val_inputs=encode_bert(test_X)\n",
        "    \n",
        "    test_label_array=test_y.values\n",
        "    test_labels=np.zeros((len(test_y),test_label_array.max()+1))\n",
        "    test_labels[np.arange(len(test_y)),test_label_array] = 1\n",
        "    \n",
        "  \n",
        "    model = create_model(optimizer,loss,acc)\n",
        "    early_stopping=tf.keras.callbacks.EarlyStopping(patience=5)\n",
        "    model.fit(train_inputs, train_labels,epochs=5,batch_size=8,\n",
        "              validation_data=(val_inputs,test_labels),callbacks=early_stopping)\n",
        "    results=model.evaluate(val_inputs,test_labels)\n",
        " \n",
        "    a+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtZaO3MHiGCA"
      },
      "outputs": [],
      "source": [
        "#for testing purposes,only the first fek as testing and the rest for training\n",
        "seq_len= 512\n",
        "d3 = pd.concat([dataframes_list[0],dataframes_list[1],dataframes_list[2],dataframes_list[3],dataframes_list[4],dataframes_list[5],dataframes_list[6],dataframes_list[7]])\n",
        "print(d3)\n",
        "train_inputs=encode_bert(d3['text'])\n",
        "print(train_inputs)\n",
        "train_y=d3['label']\n",
        "\n",
        "label_array=train_y.values\n",
        "print(label_array)\n",
        "len(label_array)\n",
        "\n",
        "train_label_array=train_y\n",
        "train_labels=np.zeros((len(train_y),train_label_array.max()+1))\n",
        "train_labels[np.arange(len(train_y)),train_label_array] = 1\n",
        "print(train_labels)\n",
        "test_inputs=encode_bert(dataframes_list[8]['text'])\n",
        "test_y=d3['label']\n",
        "test_label_array=test_y\n",
        "test_labels=np.zeros((len(test_y),test_label_array.max()+1))\n",
        "test_labels[np.arange(len(test_y)),test_label_array] = 1\n",
        "\n",
        "early_stopping=tf.keras.callbacks.EarlyStopping(patience=5)\n",
        "model = create_model(optimizer,loss,acc)\n",
        "model.fit(train_inputs, train_labels,epochs=5,batch_size=8,\n",
        "              validation_data=(test_inputs,test_labels),callbacks=early_stopping)\n",
        "results=model.evaluate(val_inputs,test_labels)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}