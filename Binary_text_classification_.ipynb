{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichelleFn/Finetuning-greek-bert-for-goverment-gazetes/blob/main/Binary_text_classification_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Installing libraries:**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8HCr3F9D3Q55"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v191elsh2T2m"
      },
      "outputs": [],
      "source": [
        "!pip install unicodedata\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install tensorflow\n",
        "!pip install nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OL9pm3dx29d0"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CpTkuMfV9k_"
      },
      "source": [
        "**Mounting csv file from google drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTFWSBeX26c9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/My Drive/csv/csv/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfrYTho_WEMI"
      },
      "source": [
        "**Loading Bert tokenizer and TensorFlow model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAEzXlZ829nR"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/bert-base-greek-uncased-v1\")\n",
        "bert = TFAutoModel.from_pretrained(\"nlpaueb/bert-base-greek-uncased-v1\",hidden_dropout_prob=0.5)\n",
        "aug = naw.ContextualWordEmbsAug(model_path='nlpaueb/bert-base-greek-uncased-v1', action=\"substitute\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stop words:**"
      ],
      "metadata": {
        "id": "m01nuLWZ4GBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words  = nltk.corpus.stopwords.words('greek')\n",
        "\n",
        "#adding extra stop words that were not included\n",
        "new_stopwords = ['της','τη','τους','ένας','ενός','ένα','μια','μιας','μιαν','αυτός','αυτή','αυτό','αυτοί','αυτά','αυτών','ούτος','αυτούς','εκείνος','εκείνη','εκείνο','εκείνοι','εκείνων','εκείνα','ποιος','ποια','ποιοι','ποιων','ποιους','ποιες','πια','είμαι','είναι','είμαστε','είστε','εγώ','εσύ','εμείς','εσείς','άλλος','άλλη','άλλο',\n",
        "'άλλων','άλλους','άλλα','κατ’','ως','ή','ούτε','ποτέ','πότε','προς','πρός','υπέρ','άμα','πέρι','οπως','όπως','από','ενώ','συν','πώς','εάν','προ','μη','ίσως','κάθε','καθε','ότι','ό,τι','όσο','στα','στους']\n",
        "\n",
        "stop_words.extend(new_stopwords)\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "Tnl8Nnmt4C3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbDb2kVIWORz"
      },
      "source": [
        "**Class for reading csv files and creating datasets:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUEaIsr63FRD"
      },
      "outputs": [],
      "source": [
        "class csv_merge:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def create_dataset(self): #creates one big dataset out of all csvs in drive \n",
        "     \n",
        "        \n",
        "        files = os.path.join(folder, \"*.csv\")\n",
        "        files = glob.glob(files)\n",
        "        temp_df = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
        "        df=self.preprocess_data(temp_df)\n",
        "        \n",
        "        return df\n",
        "\n",
        "    def preprocess_data(self,df):\n",
        "      \n",
        "      # df['RawParagraph']  = df['RawParagraph'].str.replace('[(,),.,/,\\,|,«,»,΄,#,~,!,@,#,$,%,^,&,*,_,+,=,{,},[,:,?,-]', '') #the ' and ] are missing\n",
        "      \n",
        "     \n",
        "      df[\"RawParagraph\"] = df[\"RawParagraph\"].str.lower() #makes all letters lower case since we are using Bert uncased\n",
        "\n",
        "      \n",
        "      df = df.replace(r'\\n',' ', regex=True) #removes '/n' character\n",
        "      \n",
        "      df['RespAPrediction'] = df['RespAPrediction'].replace(['Non-RespA','RespA',' RespA','RespA '],[0,1,1,1]).astype(int) #makes labels 0 or 1 (the space varient are for an error I can't find in the csvs)\n",
        "    \n",
        "      df['RawParagraph'] = df['RawParagraph'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)])) #removes stop words\n",
        "\n",
        "      \n",
        "      \n",
        "      df = df.rename(columns={'RawParagraph': 'text','RespAPrediction': 'label'})  #renames dataset collumns\n",
        "      \n",
        "      return df\n",
        "\n",
        "    def csv_dataset(self): #returns a list of datasets that contain each csv\n",
        "     \n",
        "      dataframes_list = []\n",
        "\n",
        "      list_of_names =['csva','csvb','csvc','csvd','csve','csv0','csv1','csv3','csv4','csv5','csv6','csv7']#,'csv8','csv9','csv10']#,'results20080100058','results20080100036']#,'csv11','csv12']#,'csv13','csv14','csv15']#]\n",
        "      \n",
        "      col_list=['RawParagraph','RespAPrediction'] #the column names that are neccesary\n",
        "      \n",
        "      for i in range(len(list_of_names)):\n",
        "\n",
        "        temp_df = pd.read_csv(folder+list_of_names[i]+\".csv\",usecols=col_list)\n",
        "        df=self.preprocess_data(temp_df)\n",
        "        dataframes_list.append(df)\n",
        "\n",
        "      \n",
        "\n",
        "      return dataframes_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TLV2poxWn1s"
      },
      "source": [
        "**Creating list of datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMoXnm4TOb3U"
      },
      "outputs": [],
      "source": [
        "csv_merge =  csv_merge()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ja5LMjGV3Fzp"
      },
      "outputs": [],
      "source": [
        " #Calling the methods and creating a list \n",
        "dataframes_list=csv_merge.csv_dataset()\n",
        "text=dataframes_list[0]['text']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(dataframes_list[5])\n"
      ],
      "metadata": {
        "id": "o_57o7bN9v8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding input text:**"
      ],
      "metadata": {
        "id": "a1_Oj0i06Win"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4COJyIKs2UA"
      },
      "outputs": [],
      "source": [
        "def encode_bert(text): #encoding text for Bert\n",
        "\n",
        "  seq_len= 256\n",
        "  num_samples=len(text)\n",
        "\n",
        "  #defining 2 empty numpy arrays for the input_ids and attention_mask\n",
        "  Xids=np.zeros((num_samples,seq_len))\n",
        "  Xmask=np.zeros((num_samples,seq_len))\n",
        "  \n",
        "  for i,sentence in enumerate(text):\n",
        "    \n",
        "    tokens = tokenizer.encode_plus(sentence, max_length=seq_len, truncation=True, padding='max_length',add_special_tokens=True,return_tensors='tf')\n",
        "   \n",
        "    Xids[i,:]=tokens['input_ids']\n",
        "    Xmask[i,:]=tokens['attention_mask']\n",
        "\n",
        "\n",
        "  \n",
        "  inputs = {\n",
        "      'input_ids': Xids,\n",
        "      'attention_mask': Xmask\n",
        "      }\n",
        "  \n",
        "\n",
        "  return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating data combinations**"
      ],
      "metadata": {
        "id": "ktbkFlsV5jMJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gruaUhIXfZAv"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Creating combinations of feks for 10 fold cross validation\n",
        "\n",
        "\"\"\"\n",
        "def create_dataset_combinations(fold_loop_number,all_csvs): #Splitting csvs into train and test\n",
        "  \n",
        "  x_test=0\n",
        "  y_test=0\n",
        "  df= pd.DataFrame(columns=['text','label'])\n",
        "  x_train=[]\n",
        "  y_train=[]\n",
        "  y=0\n",
        "  special_tokens_train=[]\n",
        "\n",
        "  print('FEK',fold_loop_number,'used for testing, the rest for training')\n",
        "  for csv in all_csvs: #iterating through each dataset in the list\n",
        "    if y==fold_loop_number: #choosing the test dataset\n",
        "      x_test=csv['text'].values\n",
        "      y_test=csv['label'].values\n",
        "  \n",
        "    else: #the remaining datasets are merged into one\n",
        "     \n",
        "      x_train.append(csv['text'].values)\n",
        "      y_train.append(csv['label'].values)\n",
        "\n",
        "    y=y+1\n",
        "    \n",
        "  x_train=np.concatenate(x_train, axis=0 )\n",
        "  y_train=np.concatenate(y_train,axis=0)\n",
        "  \n",
        "  return x_test,y_test,x_train,y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating model**:"
      ],
      "metadata": {
        "id": "0OQmfd2efkZg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCw2ayWz6WZm"
      },
      "outputs": [],
      "source": [
        "def create_model(optimizer,loss,acc): #Bert model\n",
        "  seq_len=256\n",
        "  input_ids=tf.keras.layers.Input(shape=(seq_len,),name='input_ids',dtype='int32')\n",
        "  mask=tf.keras.layers.Input(shape=(seq_len,),name='attention_mask',dtype='int32')\n",
        "\n",
        "  embeddings=bert.bert(input_ids,attention_mask=mask)[1]\n",
        "  # layer = tf.keras.layers.Dropout(0.4)(embeddings)\n",
        "\n",
        "  x = tf.keras.layers.Dense(512,activation='relu')(embeddings)\n",
        "  layer2 = tf.keras.layers.Dropout(0.4)(x)\n",
        "\n",
        "  y = tf.keras.layers.Dense(2,activation='sigmoid',name='outputs')(layer2)\n",
        "  model=tf.keras.Model(inputs=[input_ids,mask],outputs=y)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qnA_PM5UNzr"
      },
      "outputs": [],
      "source": [
        "optimizer= tf.keras.optimizers.Adam(lr=0.0001,decay=1e-6)\n",
        "loss=tf.keras.losses.BinaryCrossentropy()\n",
        "acc=tf.keras.metrics.BinaryAccuracy('accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErNZd39CNmYp"
      },
      "source": [
        "\n",
        " **10 fold cross validation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsQIozO9i7U9"
      },
      "outputs": [],
      "source": [
        "for i in range(9):\n",
        "  i=4\n",
        "  tf.keras.backend.clear_session()\n",
        "  x_test,y_test,x_train,y_train= create_dataset_combinations(i,dataframes_list)\n",
        "\n",
        "  print('Size of training set: ',len(x_train))\n",
        "  print('Size of test set: ', len(x_test))\n",
        "\n",
        "  val_inputs=encode_bert(x_test) \n",
        "  test_labels=np.zeros((len(y_test),y_test.max()+1))\n",
        "  test_labels[np.arange(len(y_test)),y_test] = 1\n",
        "  \n",
        "  train_inputs=encode_bert(x_train)\n",
        "\n",
        "\n",
        "  train_label_array=y_train\n",
        "  train_labels=np.zeros((len(y_train),y_train.max()+1))\n",
        "  train_labels[np.arange(len(y_train)),y_train] = 1\n",
        "  \n",
        "\n",
        "  model = create_model(optimizer,loss,acc)\n",
        "  model.fit(train_inputs, train_labels,epochs=3,batch_size=16, validation_data=(val_inputs,test_labels))\n",
        "  results=model.evaluate(val_inputs,test_labels)\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAZvZHNILsKk"
      },
      "source": [
        "**Stratified 10 cross fold validation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVYYu0LIOkc_"
      },
      "outputs": [],
      "source": [
        "df= csv_merge.create_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL6gnltConAc"
      },
      "outputs": [],
      "source": [
        "X_data=df['text']\n",
        "Y_data=df['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G28076P1nof9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "#9 fold cross validation with complete dataset that is split into train and test each time\n",
        "skf = StratifiedKFold(n_splits=10,shuffle=True, random_state=1)\n",
        "\n",
        "\n",
        "a=0\n",
        "score=[]\n",
        "for train_index, test_index in skf.split(X_data, Y_data):\n",
        "    \n",
        "    print(f\"Generating Inputs for fold {a}\")\n",
        "   \n",
        "    \n",
        "    \n",
        "    train_X, test_X = X_data[train_index], X_data[test_index]\n",
        "    train_y, test_y = Y_data[train_index], Y_data[test_index]\n",
        "\n",
        "    train_inputs=encode_bert(train_X)\n",
        "\n",
        "    \n",
        "    train_label_array=train_y.values\n",
        "\n",
        "    \n",
        "    train_labels=np.zeros((len(train_y),train_label_array.max()+1))\n",
        "    train_labels[np.arange(len(train_y)),train_label_array] = 1\n",
        "    \n",
        "\n",
        "    \n",
        "    val_inputs=encode_bert(test_X)\n",
        "    \n",
        "    test_label_array=test_y.values\n",
        "    test_labels=np.zeros((len(test_y),test_label_array.max()+1))\n",
        "    test_labels[np.arange(len(test_y)),test_label_array] = 1\n",
        "    \n",
        "  \n",
        "    model = create_model(optimizer,loss,acc)\n",
        "    early_stopping=tf.keras.callbacks.EarlyStopping(patience=5)\n",
        "    model.fit(train_inputs, train_labels,epochs=5,batch_size=8,\n",
        "              validation_data=(val_inputs,test_labels),callbacks=early_stopping)\n",
        "    results=model.evaluate(val_inputs,test_labels)\n",
        "  \n",
        "\n",
        "    \n",
        "    a+=1\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtZaO3MHiGCA"
      },
      "outputs": [],
      "source": [
        "#for testing purposes,only the first fek as testing and the rest for training\n",
        "seq_len= 512\n",
        "d3 = pd.concat([dataframes_list[0],dataframes_list[1],dataframes_list[2],dataframes_list[3],dataframes_list[4],dataframes_list[5],dataframes_list[6],dataframes_list[7]])\n",
        "print(d3)\n",
        "train_inputs=encode_bert(d3['text'])\n",
        "print(train_inputs)\n",
        "train_y=d3['label']\n",
        "\n",
        "label_array=train_y.values\n",
        "print(label_array)\n",
        "len(label_array)\n",
        "\n",
        "train_label_array=train_y\n",
        "train_labels=np.zeros((len(train_y),train_label_array.max()+1))\n",
        "train_labels[np.arange(len(train_y)),train_label_array] = 1\n",
        "print(train_labels)\n",
        "test_inputs=encode_bert(dataframes_list[8]['text'])\n",
        "test_y=d3['label']\n",
        "test_label_array=test_y\n",
        "test_labels=np.zeros((len(test_y),test_label_array.max()+1))\n",
        "test_labels[np.arange(len(test_y)),test_label_array] = 1\n",
        "\n",
        "early_stopping=tf.keras.callbacks.EarlyStopping(patience=5)\n",
        "model = create_model(optimizer,loss,acc)\n",
        "model.fit(train_inputs, train_labels,epochs=5,batch_size=8,\n",
        "              validation_data=(test_inputs,test_labels),callbacks=early_stopping)\n",
        "results=model.evaluate(val_inputs,test_labels)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}